{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:09.271406Z",
     "start_time": "2019-01-27T01:34:08.287160Z"
    },
    "autopy": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adryw/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/adryw/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Model, load_model, clone_model\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "sys.path.append('src')  # Fix for jupyter\n",
    "import src.emulator as emulator\n",
    "import src.emulator_utils as emulator_utils\n",
    "import src.emulator_vis as emulator_vis\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Event, Queue, Pipe\n",
    "from multiprocessing import Process as Thread\n",
    "import os\n",
    "import logging\n",
    "from mcts import *\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autopy": 0
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:09.275838Z",
     "start_time": "2019-01-27T01:34:09.272909Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='logging.log', level=logging.INFO, format='%(asctime)s %(message)s', filemode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autopy": 0
   },
   "source": [
    "# Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:09.280227Z",
     "start_time": "2019-01-27T01:34:09.277803Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = (16, 16, 5) # Map size fixed to 16x16 (2 to 3 players)\n",
    "N_ACTIONS = 4\n",
    "gpus = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autopy": 0
   },
   "source": [
    "# Define the Layers Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:09.290395Z",
     "start_time": "2019-01-27T01:34:09.281664Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "filters = 64\n",
    "\n",
    "# Convolutional Block\n",
    "def conv_block(in_layer, name, filters=filters, kernel_size=(3,3), bn=True, relu=True):\n",
    "    l = Conv2D(filters, kernel_size, use_bias = False, \n",
    "               padding='same', name = name, kernel_regularizer=l2(1e-4))(in_layer)\n",
    "    if bn:\n",
    "        l = BatchNormalization(axis=3, name = name + '_bn')(l)\n",
    "    if relu:\n",
    "        #l = Activation('relu', name = name + '_relu')(l)\n",
    "        l = LeakyReLU(name = name + '_lkrelu')(l)\n",
    "\n",
    "    return l\n",
    "\n",
    "# Residual Block\n",
    "def residual_conv(in_layer, idx, filters=filters, kernel_size=(3,3), bn=True, relu=True):\n",
    "    name = 'res_' + str(idx)\n",
    "    # Full conv block of pre-defined shape\n",
    "    l = conv_block(in_layer, name + '_conv1', filters, kernel_size=(3,3), bn=True, relu=True)\n",
    "    # Second block with skip connection\n",
    "    l = Conv2D(filters, kernel_size, use_bias = False, padding='same', \n",
    "               name = name + '_conv2', kernel_regularizer=l2(1e-4))(l)\n",
    "    if bn:\n",
    "        l = BatchNormalization(axis=3, name = name + '_conv2_bn')(l)\n",
    "    \n",
    "    l = Concatenate()([in_layer, l]) # Skip conn.\n",
    "    #l = Add()([in_layer, l]) # Skip conn.\n",
    "    \n",
    "    if relu:\n",
    "        #l = Activation('relu', name = name + '_relu')(l)\n",
    "        l = LeakyReLU(name = name + '_lkrelu')(l)\n",
    "        \n",
    "    return l\n",
    "\n",
    "def value_head(in_layer):\n",
    "    l = conv_block(in_layer, 'value_head', filters=1, kernel_size=(1,1))\n",
    "    l = Flatten(name = 'value_flatten')(l)\n",
    "    l = Dense(64, name = 'value_dense')(l)\n",
    "    #l = Activation('relu', name = 'value_relu')(l)\n",
    "    l = LeakyReLU(name = 'value_lkrelu')(l)\n",
    "    \n",
    "    l = BatchNormalization(axis=1, name = 'value_bn')(l)\n",
    "\n",
    "    l = Dense(1, use_bias = False, name = 'value', activation='tanh')(l) # Value output\n",
    "    return l\n",
    "\n",
    "def policy_head(in_layer):\n",
    "    l = conv_block(in_layer, 'policy_head', filters=2, kernel_size=(1,1))\n",
    "    l = Flatten(name = 'policy_flatten')(l)\n",
    "    l = Dense(N_ACTIONS, name = 'policy', use_bias = False, activation='linear')(l) # Policy output\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autopy": 0
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:09.297841Z",
     "start_time": "2019-01-27T01:34:09.293543Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    def declare_model():\n",
    "        n_residual = 6\n",
    "\n",
    "        input_layer = Input(INPUT_SIZE)\n",
    "        l = conv_block(input_layer, 'conv')\n",
    "        for i in range(n_residual):\n",
    "            l = residual_conv(l, idx=i + 1)\n",
    "\n",
    "        policy = policy_head(l)\n",
    "        \n",
    "        value = value_head(l)\n",
    "\n",
    "        alphabot = Model(input_layer, [policy, value])\n",
    "        return alphabot\n",
    "    \n",
    "    if gpus > 1:\n",
    "        with tf.device('/cpu:0'):\n",
    "            alphabot = declare_model()\n",
    "        alphabot_multi = multi_gpu_model(alphabot, gpus=gpus)\n",
    "        return alphabot_multi, alphabot\n",
    "    \n",
    "    alphabot = declare_model()\n",
    "    return alphabot, alphabot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:09.600241Z",
     "start_time": "2019-01-27T01:34:09.584218Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "def simulate_games():\n",
    "    logging.debug('Starting Threads for parallel Games')\n",
    "    \n",
    "    parallel_sim() # Parallel Games\n",
    "    while not history_buffer.full():\n",
    "        indices, states = [], []\n",
    "        if processable_buffer.qsize() < 2: # Wait until a bunch of requests are queued\n",
    "            continue\n",
    "\n",
    "        for i in range(processable_buffer.qsize()):\n",
    "            index, state = processable_buffer.get()\n",
    "            indices.append(index)\n",
    "            states.append(state)\n",
    "            \n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        predictions = alphabot.predict(states)\n",
    "        for i, pred in enumerate(tuple(zip(predictions[0], predictions[1]))):\n",
    "            pipes[indices[i]].send(dict(zip(alphabot.output_names, pred)))\n",
    "\n",
    "    logging.info('Finished Simulating %s games', n_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:10.275924Z",
     "start_time": "2019-01-27T01:34:10.268313Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "def play_eval(reverted=False):\n",
    "    global alphabot_best\n",
    "    global alphabot\n",
    "    \n",
    "    game = emulator.Game(2)\n",
    "    mapp = game.reset()\n",
    "  \n",
    "    tree_player0 = MCTS()\n",
    "    tree_player0.alpha = MCTS_eval_alpha\n",
    "  \n",
    "    tree_player1 = MCTS()\n",
    "    tree_player1.alpha = MCTS_eval_alpha\n",
    "\n",
    "    old_mapp = None\n",
    "    turn = 0\n",
    "    s = map_to_state(mapp, old_mapp, None, 0)\n",
    "    old_mapp = copy.deepcopy(mapp)\n",
    "  \n",
    "    states = []\n",
    "    policies = []\n",
    "    #reverted = np.random.random() > 0.5\n",
    "    if reverted:\n",
    "        player1 = alphabot\n",
    "        player0 = alphabot_best\n",
    "    else:\n",
    "        player0 = alphabot\n",
    "        player1 = alphabot_best\n",
    "    \n",
    "    while True:\n",
    "        states.append(np.array(s))\n",
    "        if turn == 0:\n",
    "            policy = do_search(MCTS_eval_steps, s, mapp, game, tree_player0, alphabot=player0, allow_move=allow_move)\n",
    "        else:\n",
    "            policy = do_search(MCTS_eval_steps, s, mapp, game, tree_player1, alphabot=player1, allow_move=allow_move)\n",
    "            \n",
    "        if not use_eval_choice:\n",
    "            choosen = np.argmax(policy)\n",
    "        else :\n",
    "            choosen = np.random.choice(4, p=policy)\n",
    "\n",
    "        policies.append(np.array(policy))\n",
    "        mapp = game.step(mapp, s, choosen, turn)\n",
    "\n",
    "        turn = 1 - turn\n",
    "        if turn == 0:  # We update the state\n",
    "            s = map_to_state(mapp, old_mapp, s, 0)  # TODO: Map to state\n",
    "        else:\n",
    "            s[..., -1] = 1\n",
    "\n",
    "        if turn == 0:\n",
    "            old_mapp = np.array(mapp)\n",
    "\n",
    "        if game.game_ended():\n",
    "            if not reverted:\n",
    "                return int(turn)\n",
    "            else:\n",
    "                return  int(not turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:10.853342Z",
     "start_time": "2019-01-27T01:34:10.846978Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "        picked_data = random.sample(complete_history, k=min(BATCH_SIZE, len(complete_history)))\n",
    "        \n",
    "        state = []\n",
    "        policy = []\n",
    "        value = []\n",
    "        for step in picked_data:\n",
    "            policy.append(step.policy)\n",
    "            state.append(step.state)\n",
    "            value.append(step.value)\n",
    "            \n",
    "        y = [np.zeros((len(state), 4)), np.zeros((len(state), 1))]\n",
    "        y[0] = policy\n",
    "        y[1] = value\n",
    "        \n",
    "        logging.debug('The label is %s', y)\n",
    "        losses = alphabot.train_on_batch(np.array(state, dtype=np.float32), y)\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:11.428340Z",
     "start_time": "2019-01-27T01:34:11.396084Z"
    },
    "autopy": 0,
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def training_cycle():\n",
    "    global alphabot\n",
    "    global alphabot_best\n",
    "    global total_improv\n",
    "    \n",
    "    # Simulate n_games (exception made by first interaction)\n",
    "    logging.info('Starting Training Cycle')\n",
    "    #while len(complete_history) < k * n_games:\n",
    "    simulate_games()\n",
    "    # history_buffer contains the games, we store them inside complete history    \n",
    "    for g in range(history_buffer.qsize()):\n",
    "        complete_history.append(history_buffer.get())\n",
    "    stop_simulation() # We can now stop the simulation (will free the memory)\n",
    "    logging.debug('Complete history should be full, it contains %s elements', len(complete_history))\n",
    "    # Now we are ready for the training process\n",
    "    logging.info('Starting Model Training')\n",
    "    losses = [None, None, None] # For debug purpose\n",
    "    sum_loss = 0\n",
    "    cc = 1\n",
    "    for i in range(t_steps + 1):\n",
    "        if i % 100 == 0:\n",
    "            logging.info('Training Interaction: %s losses: %s', i, \n",
    "                         round(sum_loss / cc, 2)) # Works?\n",
    "\n",
    "        losses = train_model()\n",
    "        sum_loss += losses[0]\n",
    "        logging.debug('Losses: %s', losses)\n",
    "        \n",
    "        improved = False\n",
    "        evalued_step = False\n",
    "        cc += 1\n",
    "        if i % eval_steps == 0 and i > 0:\n",
    "            evalued_step = True\n",
    "            cc = 1 # Reset loss counter\n",
    "            sum_loss = 0\n",
    "            wins = {'candidate' : 0, 'best' : 0}\n",
    "            n_c = {0 : 'candidate', 1 : 'best'}\n",
    "            \n",
    "            logging.info('Starting self-play evaluation')    \n",
    "            for j in range(eval_games):\n",
    "                if j >= eval_games // 2:\n",
    "                    reverted = True\n",
    "                else:\n",
    "                    reverted = False\n",
    "                    \n",
    "                wins[n_c[play_eval(reverted)]] += 1 # add a win to the winner\n",
    "                if j % 10 == 0:\n",
    "                    logging.info('Win state Candidate: %s Best: %s', wins['candidate'], wins['best'])\n",
    "            win_ratio = round(wins['candidate'] / eval_games, 2)\n",
    "            if win_ratio >= win_percent:\n",
    "                logging.info('Great! Our candidate won %s percent of games', win_ratio * 100)\n",
    "                total_improv += 1\n",
    "                logging.info('Our bot got better %s times', total_improv)\n",
    "                improved = True\n",
    "                alphabot.save('alphabot_best.pickle')\n",
    "                alphabot_best = load_model('alphabot_best.pickle', \n",
    "                           custom_objects={'softmax_cross_entropy_with_logits' : softmax_cross_entropy_with_logits})\n",
    "            else:\n",
    "                logging.info('Damn! Our candidate only won %s percent of games', round(win_ratio * 100, 2))         \n",
    "        if not improved and evalued_step:\n",
    "            logging.info('Not improved, cloning to best')\n",
    "            alphabot = load_model('alphabot_best.pickle', \n",
    "                           custom_objects={'softmax_cross_entropy_with_logits' : softmax_cross_entropy_with_logits})\n",
    "        \n",
    "        if improved:\n",
    "            logging.info('Already improved, simulating more games')\n",
    "            break\n",
    "            \n",
    "    if len(complete_history) == k * n_games:\n",
    "        logging.debug('Removing oldest games')\n",
    "        del complete_history[:n_games] # Delete n oldest games from history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:11.934662Z",
     "start_time": "2019-01-27T01:34:11.929749Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "def load_best(best_model):\n",
    "    global alphabot\n",
    "    global alphabot_best\n",
    "    alphabot_best = load_model(best_model, \n",
    "                               custom_objects={'softmax_cross_entropy_with_logits' : softmax_cross_entropy_with_logits})\n",
    "    alphabot.set_weights(alphabot_best.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:12.924915Z",
     "start_time": "2019-01-27T01:34:12.922222Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "def train(cycles, best_model = None):\n",
    "    global alphabot_best\n",
    "    global alphabot\n",
    "    \n",
    "    #with open(r\"alphabot_best.pickle\", \"wb\") as output_file:\n",
    "    #    pickle.dump(alphabot, output_file)\n",
    "    alphabot.save('alphabot_best.pickle')\n",
    "    \n",
    "    #with open(r\"alphabot_best.pickle\", \"rb\") as input_file:\n",
    "    #    alphabot_best = pickle.load(input_file)\n",
    "    alphabot_best = load_model('alphabot_best.pickle', \n",
    "                           custom_objects={'softmax_cross_entropy_with_logits' : softmax_cross_entropy_with_logits})\n",
    "    \n",
    "    complete_history = []\n",
    "    for i in range(cycles):\n",
    "        training_cycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:14.225678Z",
     "start_time": "2019-01-27T01:34:14.218271Z"
    },
    "autopy": 0,
    "code_folding": [
     32
    ]
   },
   "outputs": [],
   "source": [
    "def ask_predict(idi, x):\n",
    "    # Adds to queue id and data from process\n",
    "    processable_buffer.put((idi, x))\n",
    "\n",
    "def sim(process_id, pipe):\n",
    "    np.random.seed()\n",
    "    random.seed()\n",
    "    \n",
    "    while True:\n",
    "        train_steps = simulate_game(MCTS_steps, MCTS_alpha, pipe, ask_predict, process_id)    \n",
    "        \n",
    "        try:\n",
    "            for step in train_steps:\n",
    "                history_buffer.put_nowait(step)\n",
    "        except:\n",
    "            break\n",
    "                    \n",
    "def stop_simulation():\n",
    "    global workers\n",
    "    global history_buffer\n",
    "    global processable_buffer\n",
    "    global pipes\n",
    "    global child_pipes\n",
    "    \n",
    "    if 'workers' in globals() and len(workers) != 0:\n",
    "        for worker in workers:\n",
    "            worker.terminate()\n",
    "    workers = []\n",
    "    \n",
    "    for pipe in pipes:\n",
    "        pipe.close()\n",
    "\n",
    "    for pipe in child_pipes:\n",
    "        pipe.close()\n",
    "    \n",
    "    #for _ in range(history_buffer.qsize()):\n",
    "    #    try:\n",
    "    #        history_buffer.get_nowait()\n",
    "    #    except:\n",
    "    #        break\n",
    "            \n",
    "    #for _ in range(processable_buffer.qsize()):\n",
    "    #    try:\n",
    "    #        processable_buffer.get_nowait()\n",
    "    #    except:\n",
    "    #        break\n",
    "        \n",
    "    history_buffer.close()\n",
    "    processable_buffer.close()\n",
    "    \n",
    "    # Then we empty the queues\n",
    "    del history_buffer\n",
    "    del processable_buffer\n",
    "    del pipes\n",
    "    del child_pipes\n",
    "\n",
    "def parallel_sim():\n",
    "    global workers\n",
    "    global history_buffer\n",
    "    global processable_buffer\n",
    "    global pipes\n",
    "    global child_pipes\n",
    "    \n",
    "    if 'workers' in globals() and len(workers) != 0:\n",
    "        stop_simulation()\n",
    "    \n",
    "    history_buffer = Queue(n_games) # This numbers can be tweaked\n",
    "    processable_buffer = Queue(num_threads)\n",
    "    pipes = []\n",
    "    child_pipes = []\n",
    "    \n",
    "    workers = []\n",
    "    for i in range(num_threads):\n",
    "        parent_pipe, child_pipe = Pipe() # Pipe to communicate with childs\n",
    "        pipes.append(parent_pipe)\n",
    "        child_pipes.append(child_pipe)\n",
    "        \n",
    "        worker = Thread(target=sim, args=[i, child_pipe])\n",
    "        worker.daemon = False\n",
    "        worker.start()\n",
    "        workers.append(worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:15.646598Z",
     "start_time": "2019-01-27T01:34:15.642999Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def softmax_cross_entropy_with_logits(y_true, y_pred):\n",
    "\n",
    "    p = y_pred\n",
    "    pi = y_true\n",
    "\n",
    "    zero = tf.zeros(shape = tf.shape(pi), dtype=tf.float32)\n",
    "    where = tf.equal(pi, zero)\n",
    "\n",
    "    negatives = tf.fill(tf.shape(pi), -100.0) \n",
    "    p = tf.where(where, negatives, p)\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels = pi, logits = p)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:22.185171Z",
     "start_time": "2019-01-27T01:34:21.106754Z"
    },
    "autopy": 0,
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________________________________________\n",
      "Layer (type)                        Output Shape             Param #       Connected to                         \n",
      "================================================================================================================\n",
      "input_2 (InputLayer)                (None, 16, 16, 5)        0                                                  \n",
      "________________________________________________________________________________________________________________\n",
      "conv (Conv2D)                       (None, 16, 16, 64)       2880          input_2[0][0]                        \n",
      "________________________________________________________________________________________________________________\n",
      "conv_bn (BatchNormalization)        (None, 16, 16, 64)       256           conv[0][0]                           \n",
      "________________________________________________________________________________________________________________\n",
      "conv_lkrelu (LeakyReLU)             (None, 16, 16, 64)       0             conv_bn[0][0]                        \n",
      "________________________________________________________________________________________________________________\n",
      "res_1_conv1 (Conv2D)                (None, 16, 16, 64)       36864         conv_lkrelu[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "res_1_conv1_bn (BatchNormalization) (None, 16, 16, 64)       256           res_1_conv1[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "res_1_conv1_lkrelu (LeakyReLU)      (None, 16, 16, 64)       0             res_1_conv1_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_1_conv2 (Conv2D)                (None, 16, 16, 64)       36864         res_1_conv1_lkrelu[0][0]             \n",
      "________________________________________________________________________________________________________________\n",
      "res_1_conv2_bn (BatchNormalization) (None, 16, 16, 64)       256           res_1_conv2[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)         (None, 16, 16, 128)      0             conv_lkrelu[0][0]                    \n",
      "                                                                           res_1_conv2_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_1_lkrelu (LeakyReLU)            (None, 16, 16, 128)      0             concatenate_7[0][0]                  \n",
      "________________________________________________________________________________________________________________\n",
      "res_2_conv1 (Conv2D)                (None, 16, 16, 64)       73728         res_1_lkrelu[0][0]                   \n",
      "________________________________________________________________________________________________________________\n",
      "res_2_conv1_bn (BatchNormalization) (None, 16, 16, 64)       256           res_2_conv1[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "res_2_conv1_lkrelu (LeakyReLU)      (None, 16, 16, 64)       0             res_2_conv1_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_2_conv2 (Conv2D)                (None, 16, 16, 64)       36864         res_2_conv1_lkrelu[0][0]             \n",
      "________________________________________________________________________________________________________________\n",
      "res_2_conv2_bn (BatchNormalization) (None, 16, 16, 64)       256           res_2_conv2[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)         (None, 16, 16, 192)      0             res_1_lkrelu[0][0]                   \n",
      "                                                                           res_2_conv2_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_2_lkrelu (LeakyReLU)            (None, 16, 16, 192)      0             concatenate_8[0][0]                  \n",
      "________________________________________________________________________________________________________________\n",
      "res_3_conv1 (Conv2D)                (None, 16, 16, 64)       110592        res_2_lkrelu[0][0]                   \n",
      "________________________________________________________________________________________________________________\n",
      "res_3_conv1_bn (BatchNormalization) (None, 16, 16, 64)       256           res_3_conv1[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "res_3_conv1_lkrelu (LeakyReLU)      (None, 16, 16, 64)       0             res_3_conv1_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_3_conv2 (Conv2D)                (None, 16, 16, 64)       36864         res_3_conv1_lkrelu[0][0]             \n",
      "________________________________________________________________________________________________________________\n",
      "res_3_conv2_bn (BatchNormalization) (None, 16, 16, 64)       256           res_3_conv2[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)         (None, 16, 16, 256)      0             res_2_lkrelu[0][0]                   \n",
      "                                                                           res_3_conv2_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_3_lkrelu (LeakyReLU)            (None, 16, 16, 256)      0             concatenate_9[0][0]                  \n",
      "________________________________________________________________________________________________________________\n",
      "res_4_conv1 (Conv2D)                (None, 16, 16, 64)       147456        res_3_lkrelu[0][0]                   \n",
      "________________________________________________________________________________________________________________\n",
      "res_4_conv1_bn (BatchNormalization) (None, 16, 16, 64)       256           res_4_conv1[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "res_4_conv1_lkrelu (LeakyReLU)      (None, 16, 16, 64)       0             res_4_conv1_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_4_conv2 (Conv2D)                (None, 16, 16, 64)       36864         res_4_conv1_lkrelu[0][0]             \n",
      "________________________________________________________________________________________________________________\n",
      "res_4_conv2_bn (BatchNormalization) (None, 16, 16, 64)       256           res_4_conv2[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)        (None, 16, 16, 320)      0             res_3_lkrelu[0][0]                   \n",
      "                                                                           res_4_conv2_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_4_lkrelu (LeakyReLU)            (None, 16, 16, 320)      0             concatenate_10[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_5_conv1 (Conv2D)                (None, 16, 16, 64)       184320        res_4_lkrelu[0][0]                   \n",
      "________________________________________________________________________________________________________________\n",
      "res_5_conv1_bn (BatchNormalization) (None, 16, 16, 64)       256           res_5_conv1[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "res_5_conv1_lkrelu (LeakyReLU)      (None, 16, 16, 64)       0             res_5_conv1_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_5_conv2 (Conv2D)                (None, 16, 16, 64)       36864         res_5_conv1_lkrelu[0][0]             \n",
      "________________________________________________________________________________________________________________\n",
      "res_5_conv2_bn (BatchNormalization) (None, 16, 16, 64)       256           res_5_conv2[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)        (None, 16, 16, 384)      0             res_4_lkrelu[0][0]                   \n",
      "                                                                           res_5_conv2_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_5_lkrelu (LeakyReLU)            (None, 16, 16, 384)      0             concatenate_11[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_6_conv1 (Conv2D)                (None, 16, 16, 64)       221184        res_5_lkrelu[0][0]                   \n",
      "________________________________________________________________________________________________________________\n",
      "res_6_conv1_bn (BatchNormalization) (None, 16, 16, 64)       256           res_6_conv1[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "res_6_conv1_lkrelu (LeakyReLU)      (None, 16, 16, 64)       0             res_6_conv1_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_6_conv2 (Conv2D)                (None, 16, 16, 64)       36864         res_6_conv1_lkrelu[0][0]             \n",
      "________________________________________________________________________________________________________________\n",
      "res_6_conv2_bn (BatchNormalization) (None, 16, 16, 64)       256           res_6_conv2[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)        (None, 16, 16, 448)      0             res_5_lkrelu[0][0]                   \n",
      "                                                                           res_6_conv2_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "res_6_lkrelu (LeakyReLU)            (None, 16, 16, 448)      0             concatenate_12[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "value_head (Conv2D)                 (None, 16, 16, 1)        448           res_6_lkrelu[0][0]                   \n",
      "________________________________________________________________________________________________________________\n",
      "value_head_bn (BatchNormalization)  (None, 16, 16, 1)        4             value_head[0][0]                     \n",
      "________________________________________________________________________________________________________________\n",
      "value_head_lkrelu (LeakyReLU)       (None, 16, 16, 1)        0             value_head_bn[0][0]                  \n",
      "________________________________________________________________________________________________________________\n",
      "policy_head (Conv2D)                (None, 16, 16, 2)        896           res_6_lkrelu[0][0]                   \n",
      "________________________________________________________________________________________________________________\n",
      "value_flatten (Flatten)             (None, 256)              0             value_head_lkrelu[0][0]              \n",
      "________________________________________________________________________________________________________________\n",
      "policy_head_bn (BatchNormalization) (None, 16, 16, 2)        8             policy_head[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "value_dense (Dense)                 (None, 64)               16448         value_flatten[0][0]                  \n",
      "________________________________________________________________________________________________________________\n",
      "policy_head_lkrelu (LeakyReLU)      (None, 16, 16, 2)        0             policy_head_bn[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "value_lkrelu (LeakyReLU)            (None, 64)               0             value_dense[0][0]                    \n",
      "________________________________________________________________________________________________________________\n",
      "policy_flatten (Flatten)            (None, 512)              0             policy_head_lkrelu[0][0]             \n",
      "________________________________________________________________________________________________________________\n",
      "value_bn (BatchNormalization)       (None, 64)               256           value_lkrelu[0][0]                   \n",
      "________________________________________________________________________________________________________________\n",
      "policy (Dense)                      (None, 4)                2048          policy_flatten[0][0]                 \n",
      "________________________________________________________________________________________________________________\n",
      "value (Dense)                       (None, 1)                64            value_bn[0][0]                       \n",
      "================================================================================================================\n",
      "Total params: 1,021,708\n",
      "Trainable params: 1,019,910\n",
      "Non-trainable params: 1,798\n",
      "________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "alphabot, _ = create_model()\n",
    "alphabot.compile(optimizer=SGD(1e-3, momentum=0.9),\n",
    "                          loss={'value' : 'mse', 'policy' : softmax_cross_entropy_with_logits},\n",
    "                          loss_weights={'value' : 0.5, 'policy' : 0.5})\n",
    "alphabot.summary(line_length=112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T13:39:47.139422Z",
     "start_time": "2019-01-26T13:39:07.531813Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "alphabot.save('alphabot_best.pickle')\n",
    "alphabot_best = load_model('alphabot_best.pickle', \n",
    "                           custom_objects={'softmax_cross_entropy_with_logits' : softmax_cross_entropy_with_logits})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T12:10:35.538829Z",
     "start_time": "2019-01-26T12:10:27.275332Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "load_best('alphabot_best.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:35:47.426138Z",
     "start_time": "2019-01-27T01:35:47.415784Z"
    },
    "autopy": 0,
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# History of games for training\n",
    "complete_history = []\n",
    "\n",
    "# Game Params\n",
    "n_players = 2\n",
    "n_games = 5_000 # Simulate N games before each training\n",
    "k = 8 # Games to be stored n_games * K\n",
    "\n",
    "# Eval options\n",
    "allow_move = False\n",
    "use_eval_choice = True\n",
    "\n",
    "# Simulation Params\n",
    "num_threads = 6\n",
    "\n",
    "MCTS_steps = 40\n",
    "MCTS_eval_steps = 20\n",
    "MCTS_alpha = 0.6\n",
    "MCTS_eval_alpha = 0.4\n",
    "\n",
    "# Training Params\n",
    "t_steps = 2000 # Steps of training\n",
    "eval_steps = 1000 # How many steps before evaluation\n",
    "eval_games = 100 # How many games to play to evaluate how's best model\n",
    "win_percent = 0.55 # Ratio of game won to become best model\n",
    "BATCH_SIZE = 512\n",
    "total_improv = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-27T01:35:49.857Z"
    },
    "autopy": 0,
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#complete_history = []\n",
    "cycles = 1000\n",
    "\n",
    "train(cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T00:29:55.352698Z",
     "start_time": "2019-01-23T00:29:52.066305Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "K.set_value(alphabot.optimizer.lr, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T13:31:46.327064Z",
     "start_time": "2019-01-26T13:30:23.136977Z"
    },
    "autopy": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot, best = 0, 0\n",
    "for i in range(10):\n",
    "    if i < 5:\n",
    "        winner = play_eval()\n",
    "    else:\n",
    "        winner = play_eval(True)\n",
    "        \n",
    "    if winner == 0:\n",
    "        bot += 1\n",
    "    else:\n",
    "        best += 1\n",
    "\n",
    "bot, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T02:12:42.195309Z",
     "start_time": "2019-01-25T02:05:57.845973Z"
    },
    "autopy": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 69)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot, best = 0, 0\n",
    "for i in range(100):\n",
    "    winner, _ = simulate_game(10, 0.8, None, None, None, alphabot)\n",
    "    if winner == 0:\n",
    "        bot += 1\n",
    "    else:\n",
    "        best += 1\n",
    "\n",
    "bot, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T02:24:38.022875Z",
     "start_time": "2019-01-25T02:20:51.530592Z"
    },
    "autopy": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 52)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot, best = 0, 0\n",
    "for i in range(100):\n",
    "    winner, _ = simulate_game(10, 0.8, None, None, None, alphabot)\n",
    "    if winner == 0:\n",
    "        bot += 1\n",
    "    else:\n",
    "        best += 1\n",
    "\n",
    "bot, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T02:25:41.094995Z",
     "start_time": "2019-01-25T02:25:41.060573Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autopy": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autopy": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autopy": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-26T02:20:53.803Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:19:27.222183Z",
     "start_time": "2019-01-21T00:43:46.794Z"
    },
    "autopy": 0
   },
   "outputs": [],
   "source": [
    "def simmetries(state, action):\n",
    "    # There are these simmetries:\n",
    "    # +90;+180:-90;-180 degrees rotations\n",
    "    # Flips\n",
    "    \n",
    "    def rotate_state(state, rot):\n",
    "        N = INPUT_SIZE[0] - 1\n",
    "        if rot == 0: # Rotation of 0 is simple\n",
    "            return state\n",
    "        \n",
    "        new_state = np.empty(INPUT_SIZE, dtype=np.int)\n",
    "        it = np.nditer(state, flags=['multi_index'])\n",
    "        \n",
    "        if rot == 3: # Rot of 90\n",
    "            while not it.finished:\n",
    "                x, y, c = it.multi_index\n",
    "                new_state[x, y, c] = state[y, N - x, c]\n",
    "                it.iternext()\n",
    "                \n",
    "        elif rot == 2: # Rot of 180\n",
    "            while not it.finished:\n",
    "                x, y, c = it.multi_index\n",
    "                new_state[x, y, c] = state[N - x, N - y, c]\n",
    "                it.iternext()\n",
    "        elif rot == 1: # Rot of 270\n",
    "            while not it.finished:\n",
    "                x, y, c = it.multi_index\n",
    "                new_state[x, y, c] = state[N - y, x, c]\n",
    "                it.iternext()\n",
    "        return new_state\n",
    "    \n",
    "    # First we apply a random rotation simmetry\n",
    "    simmetry = random.sample([0, 1, 2, 3], 1)[0]\n",
    "    #print(simmetry * 90)\n",
    "    #print(action, simmetry)\n",
    "    action = (action + simmetry) % N_ACTIONS\n",
    "    state = rotate_state(state, simmetry)\n",
    "    return state, action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
